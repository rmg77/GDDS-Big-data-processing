{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce Word Count Example\n",
    "Various snippets of code have been downloaded from different sources including Alexandria.  These are distributed in this document in the cells below.  The first cell is my final code that I have put together using the various snippets below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.fs.Path\n",
    "import org.apache.hadoop.mapreduce.{Job, Mapper, Reducer}\n",
    "import org.apache.hadoop.mapreduce.lib.input.{FileInputFormat, FileOutputFormat}\n",
    "import org.apache.hadoop.util.GenericOptionsParser\n",
    "import scala.collection.JavaConversions._\n",
    "\n",
    "// This class performs the map operation, translating raw input into the key-value\n",
    "// pairs we will feed into our reduce operation.\n",
    "class TokenizerMapper extends Mapper[Object, Text, Text, IntWritable] {\n",
    "  val one = new IntWritable(1)\n",
    "  val word = new Text\n",
    "  \n",
    "  override\n",
    "  def map(key: Object, value: Text, context: Mapper[Object, Text, Text, IntWritable]#Context) = {\n",
    "    for (t <- value.toString().split(\"\\\\s\")) {\n",
    "      word.set(t)\n",
    "      context.write(word, one)\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "// This class performs the reduce operation, iterating over the key-value pairs\n",
    "// produced by our map operation to produce a result. In this case we just\n",
    "// calculate a simple total for each word seen.\n",
    "class IntSumReducer extends Reducer[Text, IntWritable, Text, IntWritable] {\n",
    "  override\n",
    "  def reduce(key: Text, values: java.lang.Iterable[IntWritable],\n",
    "             context: Reducer[Text, IntWritable, Text, IntWritable]#Context) = {\n",
    "    val sum = values.foldLeft(0) { (t, i) => t + i.get }\n",
    "    context.write(key, new IntWritable(sum))\n",
    "  }\n",
    "}\n",
    "\n",
    "object WordCount {\n",
    "\n",
    "  def main(args: Array[String]): Int = {\n",
    "    val conf = new Configuration()\n",
    "    val otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs\n",
    "    if (otherArgs.length != 2) {\n",
    "      println(\"Usage: wordcount <in> <out>\")\n",
    "      return 2\n",
    "    }\n",
    "    val job = new Job(conf, \"word count\")\n",
    "    job.setJarByClass(classOf[TokenizerMapper])\n",
    "    job.setMapperClass(classOf[TokenizerMapper])\n",
    "    job.setCombinerClass(classOf[IntSumReducer])\n",
    "    job.setReducerClass(classOf[IntSumReducer])\n",
    "    job.setOutputKeyClass(classOf[Text])\n",
    "    job.setOutputValueClass(classOf[IntWritable])\n",
    "    FileInputFormat.addInputPath(job, new Path(args(0)))\n",
    "    FileOutputFormat.setOutputPath(job, new Path((args(1))))\n",
    "    if (job.waitForCompletion(true)) 0 else 1\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// It appears that there are two APIs for Apache MapReduce.  The older hadoop.mapred, and\n",
    "// the newer hadoop.mapreduce.  The Alexandria documentation seems to employ the older one\n",
    "// though it is not explictly specified.  The referenced example however employs the newer\n",
    "// one, and this is the one that my code uses.\n",
    "\n",
    "//package wordcount\n",
    "\n",
    "import org.apache.hadoop.io.{IntWritable, LongWritable, Text}\n",
    "import org.apache.hadoop.mapred.{MapReduceBase, Mapper, OutputCollector, Reporter}\n",
    "import java.util.StringTokenizer\n",
    "\n",
    "// see https://github.com/milesegan/scala-hadoop-example/blob/master/WordCount.scala\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.fs.Path\n",
    "import org.apache.hadoop.mapreduce.Job\n",
    "import org.apache.hadoop.mapreduce.Mapper\n",
    "import org.apache.hadoop.mapreduce.Reducer\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n",
    "import org.apache.hadoop.util.GenericOptionsParser\n",
    "import scala.collection.JavaConversions._\n",
    "\n",
    "// see https://www.programcreek.com/scala/org.apache.hadoop.mapred.JobConf\n",
    "import org.apache.hadoop.fs.Path\n",
    "import org.apache.hadoop.mapred.JobConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Code provided in Alexandria. This uses the older hadoop.mapred library.\n",
    "class WordCountScala extends MapReduceBase with Mapper[LongWritable, Text, Text, IntWritable] {\n",
    "    val one = new IntWritable(1);\n",
    "    val word = new Text();\n",
    "    \n",
    "    def map(key: LongWritable, value: Text, output: OutputCollector[Text, IntWritable], \n",
    "            reporter: Reporter): Unit = {\n",
    "        var line = value.toString();\n",
    "        line.split(\" \").foreach(a => {word.set(a); output.collect(word, one);});\n",
    "    }\n",
    "}\n",
    "\n",
    "// compare to code at https://www.programcreek.com/scala/org.apache.hadoop.mapred.JobConf...\n",
    "\n",
    "// This class performs the map operation, translating raw input into the key-value\n",
    "// pairs we will feed into our reduce operation.\n",
    "class TokenizerMapper extends Mapper[Object, Text, Text, IntWritable] {\n",
    "  val one = new IntWritable(1)\n",
    "  val word = new Text\n",
    "  \n",
    "  override\n",
    "  def map(key: Object, value: Text, context: Mapper[Object, Text, Text, IntWritable]#Context) = {\n",
    "    for (t <- value.toString().split(\"\\\\s\")) {\n",
    "      word.set(t)\n",
    "      context.write(word, one)\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Code provided in Alexandria. This is actually Java code.\n",
    "public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, \n",
    "Text, IntWritable> {\n",
    "    public void reduce(Text key, Iterator<IntWritable> values,\n",
    "    OutputCollector<Text, IntWritable> output, Reporter reporter)\n",
    "    throws IOException {\n",
    "        int sum = 0;\n",
    "        while (values.hasNext()) {\n",
    "            sum += values.next().get();\n",
    "        }\n",
    "        output.collect(key, new IntWritable(sum));\n",
    "    }\n",
    "}\n",
    "\n",
    "// compare to code at https://www.programcreek.com/scala/org.apache.hadoop.mapred.JobConf...\n",
    "\n",
    "// This class performs the reduce operation, iterating over the key-value pairs\n",
    "// produced by our map operation to produce a result. In this case we just\n",
    "// calculate a simple total for each word seen.\n",
    "class IntSumReducer extends Reducer[Text, IntWritable, Text, IntWritable] {\n",
    "  override\n",
    "  def reduce(key: Text, values: java.lang.Iterable[IntWritable],\n",
    "             context: Reducer[Text, IntWritable, Text, IntWritable]#Context) = {\n",
    "    val sum = values.foldLeft(0) { (t, i) => t + i.get }\n",
    "    context.write(key, new IntWritable(sum))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This class configures and runs the job with the map and reduce classes we've\n",
    "// specified above.\n",
    "object WordCount {\n",
    "\n",
    "  def main(args:Array[String]):Int = {\n",
    "    val conf = new Configuration()\n",
    "    val otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs\n",
    "    if (otherArgs.length != 2) {\n",
    "      println(\"Usage: wordcount <in> <out>\")\n",
    "      return 2\n",
    "    }\n",
    "    val job = new Job(conf, \"word count\")\n",
    "    job.setJarByClass(classOf[TokenizerMapper])\n",
    "    job.setMapperClass(classOf[TokenizerMapper])\n",
    "    job.setCombinerClass(classOf[IntSumReducer])\n",
    "    job.setReducerClass(classOf[IntSumReducer])\n",
    "    job.setOutputKeyClass(classOf[Text])\n",
    "    job.setOutputValueClass(classOf[IntWritable])\n",
    "    FileInputFormat.addInputPath(job, new Path(args(0)))\n",
    "    FileOutputFormat.setOutputPath(job, new Path((args(1))))\n",
    "    if (job.waitForCompletion(true)) 0 else 1\n",
    "  }\n",
    "\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
