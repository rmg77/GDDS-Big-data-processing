{
  "paragraphs": [
    {
      "text": "#!/bin/bash\n# to run and time this code we do the followings:\n# chmod +x 28838750_q3.sh\n# time -p ./28838750_q3.sh\n\n# please download the datasets (only once) using:\n# git clone https://gist.github.com/3ee4d0a1b7251efd45581d23c9b78c84.git dataset\n\necho \"Ralph Michael Gailis\"\necho \"28838750\"\n\necho \u0027\n// start!\nsc.setLogLevel(\"error\") // adjusting the log level to only errors (ignore warnings)\n\n// 1. Create a Spark dataframe from ./dataset/iris.csv then show its content:\nval df \u003d spark.read.option(\"header\", \"true\").csv(\"./dataset/iris.csv\")\ndf.show(df.count().toInt, false)   // ensures that all rows are printed\n\n// 2- Print the data frame’s schema.\ndf.printSchema()\n\n// 3- Convert the data frame to an RDD and display its contents.\nval myRDD \u003d df.rdd\nmyRDD.collect.foreach(println)  // prints each row on a separate line\n\n// 4- Create an RDD by reading ./dataset/big.txt and verify its contents by printing the first 5 lines.\nval lines \u003d sc.textFile(\"./dataset/big.txt\")\nlines.take(5).foreach(println)\n\n// 5- Count the number of chars (including white spaces) in the text file using map and reduce functions\nval lineLengths \u003d lines.map(line \u003d\u003e line.length)\nval numchars \u003d lineLengths.reduce(_+_)\n\nprintln(\"End of Q3\")\n// end!\n:quit\n\u0027   | spark-shell\n",
      "user": "anonymous",
      "dateUpdated": "Jun 11, 2018 5:40:10 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1528699420973_-119863785",
      "id": "20180611-164340_674726253",
      "dateCreated": "Jun 11, 2018 4:43:40 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "#!/bin/bash\n# to run and time this code we do the followings:\n# chmod +x 28838750_q4.sh\n# time -p ./28838750_q4.sh\n\n# please download the datasets (only once) using:\n# git clone https://gist.github.com/3ee4d0a1b7251efd45581d23c9b78c84.git dataset\n\necho \"Ralph Michael Gailis\"\necho \"28838750\"\n\necho \u0027\n// start!\nsc.setLogLevel(\"error\") // adjusting the log level to only errors (ignore warnings) \n\n// Create an RDD by reading ./dataset/big.txt\nval lines \u003d sc.textFile(\"./dataset/big.txt\")\n\n/*\n6- Find and display the top 100 words of the ./dataset/big.txt file with their frequency ordered from the most frequent to the least.\n The vocabulary should:\n- be case-insensitive\n- not contain tokens without alphabets eg “711” or “!?” however, “1st” and “5pm” are fine.\n- not contain empty words or words contain any space in it eg “” and “   “\n\nNote:\n- ignore ‘“?!-_)({}[] symbols in the words eg “students” \u003d “student’s” \u003d “students’” and “#covfefe” \u003d “covfefe” and “stop-word”\u003dstopwords.\n- words can be tokenized with any number of whitespace characters.\n*/\n\n// import regular expression matching package\nimport scala.util.matching.Regex\n\n// a function to accept a token and return a purely lower case alphabetic string\ndef normaliseString(s: String) \u003d {\n    // define a regular expression to allow 1st, 2nd,..., 4th,..., 1am, 2am,..., 1pm, 2pm...\n    val pattern \u003d new Regex(\"(\\\\d*1st)|(\\\\d*2nd)|(\\\\d*3rd)|(\\\\d+th)|(\\\\d+am)|(\\\\d+pm)\")\n    var normalisedStr \u003d \"\"\n    \n    // The if condition below checks for an exact once only match of the regular expression. Syntax derived from:\n    //   https://stackoverflow.com/questions/3021813/how-to-check-whether-a-string-fully-matches-a-regex-in-scala\n    if (pattern.unapplySeq(s).isDefined) {\n        normalisedStr \u003d s\n    } else {  // remove all non alphabetic characters from the token\n        normalisedStr \u003d s.replaceAll(\"[^A-Za-z]\", \"\").toLowerCase()\n    }\n    normalisedStr\n}\n\n// create a long list of all tokens in the file by splitting over arbitrary whitespace (\"\\\\s+\")\nval tokenList \u003d lines.flatMap(line \u003d\u003e line.split(\"\\\\s+\"))\n\n// now convert all tokens to lowercase and filter out non-alphabetic tokens\nval wordList \u003d tokenList.map(t \u003d\u003e normaliseString(t)).filter(word \u003d\u003e word.length \u003e 0)\nwordList.persist()   // this list will be used several times, so persist in memory\n\n// now count the frequency of each distinct word\nval counts \u003d wordList.map(word \u003d\u003e (word,1)).reduceByKey(_+_)\n\n// finally sort by value and get the top 20\ncounts.sortBy(_._2, false).take(100) // same as rdd.sortBy(pair \u003d\u003e pair._2)\n\n/* \n7- Write a program which does word count of the ./dataset/big.txt file using Spark \n (note that words may be separated by more than one whitespace):\n- lowercase all letters\n- remove these stop words from the text: [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \n\"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \n\"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \n\"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \n\"herself\", \"him\", \"himself\", \"his\", \"how\",  \"i\",  \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \n\"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \n\"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"should\", \"so\", \"some\", \"such\", \n\"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \n\"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"were\", \"what\", \n\"when\", \"where\", \"where\", \"which\", \"while\", \"who\", \"who\", \"whom\", \"why\", \"with\", \"would\", \"you\", \n\"your\", \"yours\", \"yourself\", \"yourselves\" ]\n\n- ignore quotation marks, digits, #, -,  _ and / in the tokens such that \"stop-word\"  become \"stop-word\".\n- perform the word count using Spark map and reduce functions\n- print the 10 most popular words in descending order along with their counts\n- print the 10 least common words in ascending order along with their counts\n*/\n\n// A list of words to be filtered out\nval stopwords \u003d List(\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\",\n\"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\",\n\"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",  \"i\",  \"if\", \"in\", \"into\", \"is\",\n\"it\", \"its\", \"itself\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\",  \"our\", \"ours\", \"ourselves\",\n\"out\", \"over\", \"own\", \"same\", \"she\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\",\n\"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"where\",\n\"which\", \"while\", \"who\", \"who\", \"whom\", \"why\", \"with\", \"would\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\")\n\n// filter out the stopwords - use the same wordList extracted from Q6\nval interestingWords \u003d wordList.filter(word \u003d\u003e !stopwords.contains(word))\n\n// perform the map-reduce operation for word counts\nval counts \u003d interestingWords.map(word \u003d\u003e (word,1)).reduceByKey(_+_)\n\n// print out the 10 most common and 10 least common words and their counts\ncounts.sortBy(_._2, false).take(10)  // descending order\ncounts.sortBy(_._2).take(10)         // ascending order\n\nprintln(\"The End\")\n// end!\n:quit\n\u0027   | spark-shell\n",
      "user": "anonymous",
      "dateUpdated": "Jun 11, 2018 9:05:35 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1528703267146_-1383911182",
      "id": "20180611-174747_28332105",
      "dateCreated": "Jun 11, 2018 5:47:47 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import scala.util.matching.Regex\ndef normaliseString(s: String) \u003d {\n    // define a regular expression to allow 1st, 2nd,..., 4th,..., 1am, 2am,..., 1pm, 2pm...\n    val pattern \u003d new Regex(\"(\\\\d*1st)|(\\\\d*2nd)|(\\\\d*3rd)|(\\\\d+th)|(\\\\d+am)|(\\\\d+pm)\")\n    var normalisedStr \u003d \"\"\n    \n    // The if condition below checks for an exact once only match of the regular expression. Syntax derived from:\n    //   https://stackoverflow.com/questions/3021813/how-to-check-whether-a-string-fully-matches-a-regex-in-scala\n    if (pattern.unapplySeq(s).isDefined) {\n        normalisedStr \u003d s\n    } else {  // remove all non alphabetic characters from the token\n        normalisedStr \u003d s.replaceAll(\"[^A-Za-z]\", \"\").toLowerCase()\n    }\n    normalisedStr\n    //println(s\"$normalisedStr\\t1\")\n}\n\nval strList \u003d List(\"1st\", \"3pm\", \"cat\", \"dog\", \"Rex,\", \"num7\", \"835\", \"7am\", \"What!\", \"???\")\nstrList.map(normaliseString(_))\n",
      "user": "anonymous",
      "dateUpdated": "Jun 11, 2018 9:39:50 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import scala.util.matching.Regex\nnormaliseString: (s: String)String\nstrList: List[String] \u003d List(1st, 3pm, cat, dog, Rex,, num7, 835, 7am, What!, ???)\nres164: List[String] \u003d List(1st, 3pm, cat, dog, rex, num, \"\", 7am, what, \"\")\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1528710024817_1204090153",
      "id": "20180611-194024_1465503148",
      "dateCreated": "Jun 11, 2018 7:40:24 PM",
      "dateStarted": "Jun 11, 2018 9:39:50 PM",
      "dateFinished": "Jun 11, 2018 9:39:51 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "user": "anonymous",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1528710210111_-1832688588",
      "id": "20180611-194330_298899217",
      "dateCreated": "Jun 11, 2018 7:43:30 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "assessment-3",
  "id": "2DH6U2PAD",
  "angularObjects": {
    "2CY4SB1H5:shared_process": [],
    "2CXC618EC:shared_process": [],
    "2CYVBGAMQ:shared_process": [],
    "2CY3Y61TM:shared_process": [],
    "2CXRH3T8B:shared_process": [],
    "2CXTQKGUW:shared_process": [],
    "2CXCNM6ZT:shared_process": [],
    "2CXZADFKB:shared_process": [],
    "2CUZKD23K:shared_process": [],
    "2CVM42UGQ:shared_process": [],
    "2CVPRSQB6:shared_process": [],
    "2CYHQDYMU:shared_process": [],
    "2CVV5NFCM:shared_process": [],
    "2CW23H68H:shared_process": [],
    "2CY8N5QUM:shared_process": [],
    "2CVTBZ9KW:shared_process": [],
    "2CWGDUPZW:shared_process": [],
    "2CVKDDERZ:shared_process": [],
    "2CYGMRJ4Q:shared_process": []
  },
  "config": {},
  "info": {}
}