{
  "paragraphs": [
    {
      "text": "#!/bin/bash\n\necho \"\nscan \u0027sales\u0027, {COLUMNS \u003d\u003e \u0027transaction:Product\u0027, FILTER \u003d\u003e \"\\\"\"ValueFilter(\u003d, \u0027binary:Product2\u0027)\"\\\"\", LIMIT \u003d\u003e 5}\n\" | hbase shell | awk -F, \u0027{print $NF}\u0027 | awk -F\u003d \u0027{if ($1\u003d\u003d\" value\") print $2}\u0027",
      "user": "anonymous",
      "dateUpdated": "May 27, 2018 12:41:26 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Product2\nProduct2\nProduct2\nProduct2\nProduct2\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1527334893847_-808724471",
      "id": "20180526-214133_64244292",
      "dateCreated": "May 26, 2018 9:41:33 PM",
      "dateStarted": "May 27, 2018 12:32:07 AM",
      "dateFinished": "May 27, 2018 12:32:13 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "\n#!/bin/bash\n\nfunction getVals {\n    echo \"scan \u0027sales\u0027, {COLUMNS \u003d\u003e \u0027transaction:Price\u0027, LIMIT \u003d\u003e 10}\n    \" | hbase shell | awk -F, \u0027{print $NF}\u0027 | awk -F\u003d \u0027{if ($1\u003d\u003d\" value\") print$2}\u0027\n}\n\n# output of getVals is a list of numbers. Make this into an array:\nvals\u003d($(getVals))\necho \"${vals[@]}\"\n\n# Now loop over array of values and sum\nsum\u003d0\nfor i in \"${vals[@]}\"\ndo\n# Zeppelin is not interpreting variable values properly.\n# This code works fine when run from a shell script.\n   (( sum +\u003d $i ))\n   #sum\u003d`expr $sum + 1`\n   #sum\u003d$((sum+$i))\ndone\necho \"sum \u003d $sum\"\n",
      "user": "anonymous",
      "dateUpdated": "May 28, 2018 12:16:18 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {
          "vals[@]": "",
          "vals[2]": "",
          "vals[4": "",
          "vals[4]": ""
        },
        "forms": {
          "vals[@]": {
            "name": "vals[@]",
            "defaultValue": "",
            "hidden": false
          }
        }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nsum \u003d 0\nbash: line 12: ((: sum +\u003d : syntax error: operand expected (error token is \"+\u003d \")\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1527345620587_-1088700272",
      "id": "20180527-004020_1839971789",
      "dateCreated": "May 27, 2018 12:40:20 AM",
      "dateStarted": "May 28, 2018 12:07:39 PM",
      "dateFinished": "May 28, 2018 12:07:45 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "awk -F, \u0027{for(i\u003d1; i\u003c\u003dNF; i++) if(i!\u003dNF) {printf \"col:%s,\", $i} else {printf \"col:%s\", $i}; print NL}\u0027 text.csv\n| awk -F\u003d \u0027{print $4}\u0027\nawk \u0027($3\u003d\u003d\"snow\" || $3\u003d\u003d\"snowman\") {print}\u0027 dummy_file\n| awk -F\u003d \u0027($3\u003d\u003d\"value\") {print}\u0027\nawk -F\u003d \u0027{if ($1\u003d\u003d\"value\") print $2}\u0027\nawk -F, \u0027$5 ~ /true/\u0027 file  # \u0027~\u0027 is match operator - contains string, but not exactly equal to",
      "user": "anonymous",
      "dateUpdated": "May 27, 2018 12:30:53 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1527335690932_108682183",
      "id": "20180526-215450_1774094754",
      "dateCreated": "May 26, 2018 9:54:50 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "#!/bin/bash\n\n# extract column headers from the csv file and store in header file\nhead -n 1 ~/week3/SalesJan2009.csv \u003e ~/week3/header.txt\n\n# process column header to prepare for HBase format columns -\n#   add tag \"col:\" for the column family name and set the row key\nawk -F, \u0027{\n    for(i\u003d1; i\u003c\u003dNF; i++)\n      if(i\u003d\u003d1) {printf \"HBASE_ROW_KEY,\"}\n      else if(i\u003d\u003dNF) {printf \"col:%s\", $i}\n      else {printf \"col:%s,\", $i};\n    print NL\n}\u0027 ~/week3/header.txt \u003e ~/week3/salesHeader.txt\necho \"HBase table header:\"\ncat ~/week3/salesHeader.txt\n\n# extract rest of file as the data file\ntail -n +2 ~/week3/SalesJan2009.csv \u003e ~/week3/salesData.csv\necho \"Extracted data file.\"\n\n# load the data onto the Hadoop file system\n#hadoop fs -mkdir -p /tmp/hbase\necho \"Loading data file to hdfs...\"\nhadoop fs -put ~/week3/salesData.csv /tmp/hbase\nhadoop fs -ls\n\n# now load the data file to create the HBase table, using the formatted column header file\necho \"create \u0027sales2\u0027, \u0027col\u0027\" | hbase shell\nhbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns\u003d\"$(cat ~/week3/salesHeader.txt)\" -Dimporttsv.separator\u003d\u0027,\u0027 sales2 /tmp/hbase/salesData.csv\n",
      "user": "anonymous",
      "dateUpdated": "May 28, 2018 12:31:03 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Ralph Michael Gailis\n22838750\nHBase table header:\nHBASE_ROW_KEY,col:Product,col:Price,col:Payment_Type,col:Name,col:City,col:State,col:Country,col:Account_Created,col:Last_Login,col:Latitude,col:Longitude\nExtracted data file.\nLoading data file to hdfs...\nFound 1 items\ndrwxr-xr-x   - rgai0001 supergroup          0 2018-05-13 21:09 newDir\nHBase Shell; enter \u0027help\u003cRETURN\u003e\u0027 for list of supported commands.\nType \"exit\u003cRETURN\u003e\" to leave the HBase Shell\nVersion 1.2.4, r67592f3d062743907f8c5ae00dbbe1ae4f69e5af, Tue Oct 25 18:10:20 CDT 2016\n\ncreate \u0027sales2\u0027, \u0027col\u0027\n0 row(s) in 2.4270 seconds\n\nHbase::Table - sales2\n2018-05-27 22:09:24,709 INFO  [main] zookeeper.RecoverableZooKeeper: Process identifier\u003dhconnection-0x1165b38 connecting to ZooKeeper ensemble\u003dlocalhost:2181\n2018-05-27 22:09:24,715 INFO  [main] zookeeper.ZooKeeper: Client environment:zookeeper.version\u003d3.4.6-1569965, built on 02/20/2014 09:09 GMT\n2018-05-27 22:09:24,715 INFO  [main] zookeeper.ZooKeeper: Client environment:host.name\u003dlocalhost\n2018-05-27 22:09:24,715 INFO  [main] zookeeper.ZooKeeper: Client environment:java.version\u003d1.8.0_171\n2018-05-27 22:09:24,715 INFO  [main] zookeeper.ZooKeeper: Client environment:java.vendor\u003dOracle Corporation\n2018-05-27 22:09:24,715 INFO  [main] zookeeper.ZooKeeper: Client environment:java.home\u003d/usr/lib/jvm/java-8-openjdk-amd64/jre\n2018-05-27 22:09:24,716 INFO  [main] zookeeper.ZooKeeper: Client environment:java.class.path\u003d/srv/home/rgai0001/hbase-1.2.4/bin/../conf:/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/srv/home/rgai0001/hbase-1.2.4/bin/..:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/activation-1.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/aopalliance-1.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/apacheds-i18n-2.0.0-M15.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/apacheds-kerberos-codec-2.0.0-M15.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/api-asn1-api-1.0.0-M20.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/api-util-1.0.0-M20.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/asm-3.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/avro-1.7.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-beanutils-1.7.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-beanutils-core-1.8.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-cli-1.2.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-codec-1.9.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-collections-3.2.2.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-compress-1.4.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-configuration-1.6.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-daemon-1.0.13.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-digester-1.8.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-el-1.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-httpclient-3.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-io-2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-lang-2.6.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-logging-1.2.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-math-2.2.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-math3-3.1.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-net-3.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/disruptor-3.3.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/findbugs-annotations-1.3.9-1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/guava-12.0.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/guice-3.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/guice-servlet-3.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-annotations-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-auth-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-client-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-common-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-hdfs-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-mapreduce-client-app-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-mapreduce-client-common-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-mapreduce-client-core-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-mapreduce-client-jobclient-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-mapreduce-client-shuffle-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-yarn-api-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-yarn-client-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-yarn-common-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-yarn-server-common-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-annotations-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-annotations-1.2.4-tests.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-client-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-common-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-common-1.2.4-tests.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-examples-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-external-blockcache-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-hadoop2-compat-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-hadoop-compat-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-it-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-it-1.2.4-tests.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-prefix-tree-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-procedure-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-protocol-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-resource-bundle-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-rest-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-server-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-server-1.2.4-tests.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-shell-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-thrift-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/htrace-core-3.1.0-incubating.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/httpclient-4.2.5.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/httpcore-4.4.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jackson-core-asl-1.9.13.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jackson-jaxrs-1.9.13.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jackson-mapper-asl-1.9.13.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jackson-xc-1.9.13.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jamon-runtime-2.4.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jasper-compiler-5.5.23.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jasper-runtime-5.5.23.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/javax.inject-1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/java-xmlbuilder-0.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jaxb-api-2.2.2.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jaxb-impl-2.2.3-1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jcodings-1.0.8.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jersey-client-1.9.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jersey-core-1.9.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jersey-guice-1.9.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jersey-json-1.9.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jersey-server-1.9.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jets3t-0.9.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jettison-1.3.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jetty-6.1.26.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jetty-sslengine-6.1.26.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jetty-util-6.1.26.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/joni-2.1.2.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jruby-complete-1.6.8.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jsch-0.1.42.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jsp-2.1-6.1.14.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jsp-api-2.1-6.1.14.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/junit-4.12.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/leveldbjni-all-1.8.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/libthrift-0.9.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/log4j-1.2.17.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/metrics-core-2.2.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/netty-all-4.0.23.Final.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/paranamer-2.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/protobuf-java-2.5.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/servlet-api-2.5-6.1.14.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/servlet-api-2.5.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/slf4j-api-1.7.7.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/snappy-java-1.0.4.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/spymemcached-2.11.6.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/xmlenc-0.52.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/xz-1.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/zookeeper-3.4.6.jar:/home/user/hadoop-2.7.3/etc/hadoop:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/user/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/user/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/user/hadoop-2.7.3/contrib/capacity-scheduler/*.jar\n2018-05-27 22:09:24,716 INFO  [main] zookeeper.ZooKeeper: Client environment:java.library.path\u003d/home/user/hadoop-2.7.3/lib/native\n2018-05-27 22:09:24,716 INFO  [main] zookeeper.ZooKeeper: Client environment:java.io.tmpdir\u003d/tmp\n2018-05-27 22:09:24,716 INFO  [main] zookeeper.ZooKeeper: Client environment:java.compiler\u003d\u003cNA\u003e\n2018-05-27 22:09:24,716 INFO  [main] zookeeper.ZooKeeper: Client environment:os.name\u003dLinux\n2018-05-27 22:09:24,716 INFO  [main] zookeeper.ZooKeeper: Client environment:os.arch\u003damd64\n2018-05-27 22:09:24,717 INFO  [main] zookeeper.ZooKeeper: Client environment:os.version\u003d4.4.0-121-generic\n2018-05-27 22:09:24,717 INFO  [main] zookeeper.ZooKeeper: Client environment:user.name\u003drgai0001\n2018-05-27 22:09:24,717 INFO  [main] zookeeper.ZooKeeper: Client environment:user.home\u003d/srv/home/rgai0001\n2018-05-27 22:09:24,717 INFO  [main] zookeeper.ZooKeeper: Client environment:user.dir\u003d/home/user\n2018-05-27 22:09:24,717 INFO  [main] zookeeper.ZooKeeper: Initiating client connection, connectString\u003dlocalhost:2181 sessionTimeout\u003d90000 watcher\u003dhconnection-0x1165b380x0, quorum\u003dlocalhost:2181, baseZNode\u003d/hbase\n2018-05-27 22:09:24,736 INFO  [main-SendThread(localhost:2181)] zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n2018-05-27 22:09:24,743 INFO  [main-SendThread(localhost:2181)] zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session\n2018-05-27 22:09:24,749 INFO  [main-SendThread(localhost:2181)] zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid \u003d 0x1639a86e2000041, negotiated timeout \u003d 90000\n2018-05-27 22:09:25,675 INFO  [main] Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n2018-05-27 22:09:25,735 INFO  [main] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid\u003d0x1639a86e2000041\n2018-05-27 22:09:25,738 INFO  [main] zookeeper.ZooKeeper: Session: 0x1639a86e2000041 closed\n2018-05-27 22:09:25,738 INFO  [main-EventThread] zookeeper.ClientCnxn: EventThread shut down\n2018-05-27 22:09:25,756 INFO  [main] Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n2018-05-27 22:09:25,756 INFO  [main] jvm.JvmMetrics: Initializing JVM Metrics with processName\u003dJobTracker, sessionId\u003d\n2018-05-27 22:09:25,793 INFO  [main] Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n2018-05-27 22:09:26,077 INFO  [main] input.FileInputFormat: Total input paths to process : 1\n2018-05-27 22:09:26,130 INFO  [main] mapreduce.JobSubmitter: number of splits:1\n2018-05-27 22:09:26,136 INFO  [main] Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n2018-05-27 22:09:26,247 INFO  [main] mapreduce.JobSubmitter: Submitting tokens for job: job_local690826965_0001\n2018-05-27 22:09:26,655 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527422966318/hbase-common-1.2.4.jar \u003c- /home/user/hbase-common-1.2.4.jar\n2018-05-27 22:09:26,713 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/hbase-common-1.2.4.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966318/hbase-common-1.2.4.jar\n2018-05-27 22:09:26,713 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527422966319/hbase-prefix-tree-1.2.4.jar \u003c- /home/user/hbase-prefix-tree-1.2.4.jar\n2018-05-27 22:09:26,715 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/hbase-prefix-tree-1.2.4.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966319/hbase-prefix-tree-1.2.4.jar\n2018-05-27 22:09:26,715 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527422966320/hbase-protocol-1.2.4.jar \u003c- /home/user/hbase-protocol-1.2.4.jar\n2018-05-27 22:09:26,721 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/hbase-protocol-1.2.4.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966320/hbase-protocol-1.2.4.jar\n2018-05-27 22:09:26,721 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527422966321/hbase-client-1.2.4.jar \u003c- /home/user/hbase-client-1.2.4.jar\n2018-05-27 22:09:26,728 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/hbase-client-1.2.4.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966321/hbase-client-1.2.4.jar\n2018-05-27 22:09:26,728 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527422966322/zookeeper-3.4.6.jar \u003c- /home/user/zookeeper-3.4.6.jar\n2018-05-27 22:09:26,737 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/zookeeper-3.4.6.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966322/zookeeper-3.4.6.jar\n2018-05-27 22:09:26,737 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527422966323/hbase-hadoop-compat-1.2.4.jar \u003c- /home/user/hbase-hadoop-compat-1.2.4.jar\n2018-05-27 22:09:26,739 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/hbase-hadoop-compat-1.2.4.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966323/hbase-hadoop-compat-1.2.4.jar\n2018-05-27 22:09:26,739 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527422966324/protobuf-java-2.5.0.jar \u003c- /home/user/protobuf-java-2.5.0.jar\n2018-05-27 22:09:26,743 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/protobuf-java-2.5.0.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966324/protobuf-java-2.5.0.jar\n2018-05-27 22:09:26,743 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527422966325/htrace-core-3.1.0-incubating.jar \u003c- /home/user/htrace-core-3.1.0-incubating.jar\n2018-05-27 22:09:26,745 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/htrace-core-3.1.0-incubating.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966325/htrace-core-3.1.0-incubating.jar\n2018-05-27 22:09:26,745 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527422966326/hadoop-mapreduce-client-core-2.7.3.jar \u003c- /home/user/hadoop-mapreduce-client-core-2.7.3.jar\n2018-05-27 22:09:26,747 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/hadoop-mapreduce-client-core-2.7.3.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966326/hadoop-mapreduce-client-core-2.7.3.jar\n2018-05-27 22:09:26,787 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527422966327/guava-12.0.1.jar \u003c- /home/user/guava-12.0.1.jar\n2018-05-27 22:09:26,788 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/guava-12.0.1.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966327/guava-12.0.1.jar\n2018-05-27 22:09:26,788 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527422966328/metrics-core-2.2.0.jar \u003c- /home/user/metrics-core-2.2.0.jar\n2018-05-27 22:09:26,790 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/metrics-core-2.2.0.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966328/metrics-core-2.2.0.jar\n2018-05-27 22:09:26,790 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527422966329/netty-all-4.0.23.Final.jar \u003c- /home/user/netty-all-4.0.23.Final.jar\n2018-05-27 22:09:26,795 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/netty-all-4.0.23.Final.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966329/netty-all-4.0.23.Final.jar\n2018-05-27 22:09:26,795 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527422966330/hbase-server-1.2.4.jar \u003c- /home/user/hbase-server-1.2.4.jar\n2018-05-27 22:09:26,797 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/hbase-server-1.2.4.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966330/hbase-server-1.2.4.jar\n2018-05-27 22:09:26,797 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527422966331/hadoop-common-2.7.3.jar \u003c- /home/user/hadoop-common-2.7.3.jar\n2018-05-27 22:09:26,799 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/hadoop-common-2.7.3.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966331/hadoop-common-2.7.3.jar\n2018-05-27 22:09:26,838 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966318/hbase-common-1.2.4.jar\n2018-05-27 22:09:26,838 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966319/hbase-prefix-tree-1.2.4.jar\n2018-05-27 22:09:26,839 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966320/hbase-protocol-1.2.4.jar\n2018-05-27 22:09:26,839 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966321/hbase-client-1.2.4.jar\n2018-05-27 22:09:26,839 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966322/zookeeper-3.4.6.jar\n2018-05-27 22:09:26,839 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966323/hbase-hadoop-compat-1.2.4.jar\n2018-05-27 22:09:26,839 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966324/protobuf-java-2.5.0.jar\n2018-05-27 22:09:26,839 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966325/htrace-core-3.1.0-incubating.jar\n2018-05-27 22:09:26,839 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966326/hadoop-mapreduce-client-core-2.7.3.jar\n2018-05-27 22:09:26,839 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966327/guava-12.0.1.jar\n2018-05-27 22:09:26,839 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966328/metrics-core-2.2.0.jar\n2018-05-27 22:09:26,839 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966329/netty-all-4.0.23.Final.jar\n2018-05-27 22:09:26,839 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966330/hbase-server-1.2.4.jar\n2018-05-27 22:09:26,839 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527422966331/hadoop-common-2.7.3.jar\n2018-05-27 22:09:26,843 INFO  [main] mapreduce.Job: The url to track the job: http://localhost:8080/\n2018-05-27 22:09:26,844 INFO  [main] mapreduce.Job: Running job: job_local690826965_0001\n2018-05-27 22:09:26,847 INFO  [Thread-26] mapred.LocalJobRunner: OutputCommitter set in config null\n2018-05-27 22:09:26,896 INFO  [Thread-26] Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n2018-05-27 22:09:26,898 INFO  [Thread-26] mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.hbase.mapreduce.TableOutputCommitter\n2018-05-27 22:09:26,929 INFO  [Thread-26] mapred.LocalJobRunner: Waiting for map tasks\n2018-05-27 22:09:26,944 INFO  [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local690826965_0001_m_000000_0\n2018-05-27 22:09:26,992 INFO  [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n2018-05-27 22:09:26,996 INFO  [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: hdfs://localhost:9000/tmp/hbase/salesData.csv:0+123523\n2018-05-27 22:09:27,004 INFO  [LocalJobRunner Map Task Executor #0] zookeeper.RecoverableZooKeeper: Process identifier\u003dhconnection-0x21fe44d7 connecting to ZooKeeper ensemble\u003dlocalhost:2181\n2018-05-27 22:09:27,004 INFO  [LocalJobRunner Map Task Executor #0] zookeeper.ZooKeeper: Initiating client connection, connectString\u003dlocalhost:2181 sessionTimeout\u003d90000 watcher\u003dhconnection-0x21fe44d70x0, quorum\u003dlocalhost:2181, baseZNode\u003d/hbase\n2018-05-27 22:09:27,005 INFO  [LocalJobRunner Map Task Executor #0-SendThread(localhost:2181)] zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n2018-05-27 22:09:27,005 INFO  [LocalJobRunner Map Task Executor #0-SendThread(localhost:2181)] zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session\n2018-05-27 22:09:27,008 INFO  [LocalJobRunner Map Task Executor #0-SendThread(localhost:2181)] zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid \u003d 0x1639a86e2000042, negotiated timeout \u003d 90000\n2018-05-27 22:09:27,012 INFO  [LocalJobRunner Map Task Executor #0] mapreduce.TableOutputFormat: Created table instance for sales2\n2018-05-27 22:09:27,046 INFO  [LocalJobRunner Map Task Executor #0] zookeeper.RecoverableZooKeeper: Process identifier\u003dhconnection-0x6d84670f connecting to ZooKeeper ensemble\u003dlocalhost:2181\n2018-05-27 22:09:27,047 INFO  [LocalJobRunner Map Task Executor #0] zookeeper.ZooKeeper: Initiating client connection, connectString\u003dlocalhost:2181 sessionTimeout\u003d90000 watcher\u003dhconnection-0x6d84670f0x0, quorum\u003dlocalhost:2181, baseZNode\u003d/hbase\n2018-05-27 22:09:27,048 INFO  [LocalJobRunner Map Task Executor #0-SendThread(localhost:2181)] zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n2018-05-27 22:09:27,048 INFO  [LocalJobRunner Map Task Executor #0-SendThread(localhost:2181)] zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session\n2018-05-27 22:09:27,050 INFO  [LocalJobRunner Map Task Executor #0-SendThread(localhost:2181)] zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid \u003d 0x1639a86e2000043, negotiated timeout \u003d 90000\n2018-05-27 22:09:27,062 ERROR [LocalJobRunner Map Task Executor #0] mapreduce.DefaultVisibilityExpressionResolver: Error scanning \u0027labels\u0027 table\norg.apache.hadoop.hbase.TableNotFoundException: hbase:labels\n\tat org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1283)\n\tat org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1181)\n\tat org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:305)\n\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:156)\n\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:60)\n\tat org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:210)\n\tat org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:327)\n\tat org.apache.hadoop.hbase.client.ClientScanner.nextScanner(ClientScanner.java:302)\n\tat org.apache.hadoop.hbase.client.ClientScanner.initializeScannerInConstruction(ClientScanner.java:167)\n\tat org.apache.hadoop.hbase.client.ClientScanner.\u003cinit\u003e(ClientScanner.java:162)\n\tat org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:797)\n\tat org.apache.hadoop.hbase.mapreduce.DefaultVisibilityExpressionResolver.init(DefaultVisibilityExpressionResolver.java:91)\n\tat org.apache.hadoop.hbase.mapreduce.CellCreator.\u003cinit\u003e(CellCreator.java:48)\n\tat org.apache.hadoop.hbase.mapreduce.TsvImporterMapper.setup(TsvImporterMapper.java:108)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n2018-05-27 22:09:27,069 INFO  [LocalJobRunner Map Task Executor #0] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid\u003d0x1639a86e2000043\n2018-05-27 22:09:27,074 INFO  [LocalJobRunner Map Task Executor #0] zookeeper.ZooKeeper: Session: 0x1639a86e2000043 closed\n2018-05-27 22:09:27,074 INFO  [LocalJobRunner Map Task Executor #0-EventThread] zookeeper.ClientCnxn: EventThread shut down\nBad line at offset: 69494:\nExcessive columns\n2018-05-27 22:09:27,217 INFO  [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: \n2018-05-27 22:09:27,501 INFO  [LocalJobRunner Map Task Executor #0] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid\u003d0x1639a86e2000042\n2018-05-27 22:09:27,502 INFO  [LocalJobRunner Map Task Executor #0] zookeeper.ZooKeeper: Session: 0x1639a86e2000042 closed\n2018-05-27 22:09:27,502 INFO  [LocalJobRunner Map Task Executor #0-EventThread] zookeeper.ClientCnxn: EventThread shut down\n2018-05-27 22:09:27,512 INFO  [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local690826965_0001_m_000000_0 is done. And is in the process of committing\n2018-05-27 22:09:27,520 INFO  [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map\n2018-05-27 22:09:27,521 INFO  [LocalJobRunner Map Task Executor #0] mapred.Task: Task \u0027attempt_local690826965_0001_m_000000_0\u0027 done.\n2018-05-27 22:09:27,521 INFO  [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local690826965_0001_m_000000_0\n2018-05-27 22:09:27,521 INFO  [Thread-26] mapred.LocalJobRunner: map task executor complete.\n2018-05-27 22:09:27,846 INFO  [main] mapreduce.Job: Job job_local690826965_0001 running in uber mode : false\n2018-05-27 22:09:27,847 INFO  [main] mapreduce.Job:  map 100% reduce 0%\n2018-05-27 22:09:27,849 INFO  [main] mapreduce.Job: Job job_local690826965_0001 completed successfully\n2018-05-27 22:09:27,866 INFO  [main] mapreduce.Job: Counters: 21\n\tFile System Counters\n\t\tFILE: Number of bytes read\u003d26187113\n\t\tFILE: Number of bytes written\u003d26755464\n\t\tFILE: Number of read operations\u003d0\n\t\tFILE: Number of large read operations\u003d0\n\t\tFILE: Number of write operations\u003d0\n\t\tHDFS: Number of bytes read\u003d123523\n\t\tHDFS: Number of bytes written\u003d0\n\t\tHDFS: Number of read operations\u003d3\n\t\tHDFS: Number of large read operations\u003d0\n\t\tHDFS: Number of write operations\u003d0\n\tMap-Reduce Framework\n\t\tMap input records\u003d998\n\t\tMap output records\u003d997\n\t\tInput split bytes\u003d110\n\t\tSpilled Records\u003d0\n\t\tFailed Shuffles\u003d0\n\t\tMerged Map outputs\u003d0\n\t\tGC time elapsed (ms)\u003d8\n\t\tTotal committed heap usage (bytes)\u003d62849024\n\tImportTsv\n\t\tBad Lines\u003d1\n\tFile Input Format Counters \n\t\tBytes Read\u003d123523\n\tFile Output Format Counters \n\t\tBytes Written\u003d0\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1527408311596_-364169202",
      "id": "20180527-180511_345178250",
      "dateCreated": "May 27, 2018 6:05:11 PM",
      "dateStarted": "May 27, 2018 10:09:10 PM",
      "dateFinished": "May 27, 2018 10:09:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "# extract rest of file as the data file, and add a new first column with entries \"row\u003cnumber\u003e\"\n#tail -n +2 ~/week3/SalesJan2009.csv | awk \u0027{printf \"row%i,%s\", $NR, $0}\u0027 \u003e ~/week3/salesData.csv\ntail -n +2 ~/week3/SalesJan2009.csv | nl -s \u0027,\u0027 \u003e ~/week3/salesData.csv\necho \"Extracted data file.\"",
      "user": "anonymous",
      "dateUpdated": "May 28, 2018 1:09:57 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Extracted data file.\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1527475887370_-1520507754",
      "id": "20180528-125127_741253347",
      "dateCreated": "May 28, 2018 12:51:27 PM",
      "dateStarted": "May 28, 2018 1:09:57 PM",
      "dateFinished": "May 28, 2018 1:09:57 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "#!/bin/bash\n\n# This script is the same as the one directly above, except this time\n# an extra row key column is added as the first column of the data file.\n\n# extract column headers from the csv file and store in header file\nhead -n 1 ~/week3/SalesJan2009.csv \u003e ~/week3/header.txt\n\n# process column header to prepare for HBase format columns -\n#   add tag \"col:\" for the column family name and set the row key\nawk -F, \u0027{\n    for(i\u003d0; i\u003c\u003dNF; i++)\n      if(i\u003d\u003d0) {printf \"HBASE_ROW_KEY,\"}\n      else if(i\u003d\u003dNF) {printf \"col:%s\", $i}\n      else {printf \"col:%s,\", $i};\n    print NL\n}\u0027 ~/week3/header.txt \u003e ~/week3/salesHeader.txt\necho \"HBase table header:\"\ncat ~/week3/salesHeader.txt\n\n# extract rest of file as the data file, and add a new first column as a simple numbered row key\ntail -n +2 ~/week3/SalesJan2009.csv | nl -s \u0027,\u0027 \u003e ~/week3/salesData.csv\necho \"Extracted data file.\"\n\n# load the data onto the Hadoop file system\n#hadoop fs -mkdir -p /tmp/hbase\necho \"Loading data file to hdfs...\"\nhadoop fs -put ~/week3/salesData.csv /tmp/hbase\nhadoop fs -ls\n\n# now load the data file to create the HBase table, using the formatted column header file\necho \"create \u0027sales3\u0027, \u0027col\u0027\" | hbase shell\nhbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns\u003d\"$(cat ~/week3/salesHeader.txt)\" -Dimporttsv.separator\u003d\u0027,\u0027 sales3 /tmp/hbase/salesData.csv",
      "user": "anonymous",
      "dateUpdated": "May 28, 2018 1:23:44 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "HBase table header:\nHBASE_ROW_KEY,col:Transaction_date,col:Product,col:Price,col:Payment_Type,col:Name,col:City,col:State,col:Country,col:Account_Created,col:Last_Login,col:Latitude,col:Longitude\nExtracted data file.\nLoading data file to hdfs...\nFound 1 items\ndrwxr-xr-x   - rgai0001 supergroup          0 2018-05-13 21:09 newDir\nHBase Shell; enter \u0027help\u003cRETURN\u003e\u0027 for list of supported commands.\nType \"exit\u003cRETURN\u003e\" to leave the HBase Shell\nVersion 1.2.4, r67592f3d062743907f8c5ae00dbbe1ae4f69e5af, Tue Oct 25 18:10:20 CDT 2016\n\ncreate \u0027sales3\u0027, \u0027col\u0027\n0 row(s) in 2.4360 seconds\n\nHbase::Table - sales3\n2018-05-28 13:21:42,055 INFO  [main] zookeeper.RecoverableZooKeeper: Process identifier\u003dhconnection-0x1165b38 connecting to ZooKeeper ensemble\u003dlocalhost:2181\n2018-05-28 13:21:42,062 INFO  [main] zookeeper.ZooKeeper: Client environment:zookeeper.version\u003d3.4.6-1569965, built on 02/20/2014 09:09 GMT\n2018-05-28 13:21:42,062 INFO  [main] zookeeper.ZooKeeper: Client environment:host.name\u003dlocalhost\n2018-05-28 13:21:42,062 INFO  [main] zookeeper.ZooKeeper: Client environment:java.version\u003d1.8.0_171\n2018-05-28 13:21:42,062 INFO  [main] zookeeper.ZooKeeper: Client environment:java.vendor\u003dOracle Corporation\n2018-05-28 13:21:42,063 INFO  [main] zookeeper.ZooKeeper: Client environment:java.home\u003d/usr/lib/jvm/java-8-openjdk-amd64/jre\n2018-05-28 13:21:42,063 INFO  [main] zookeeper.ZooKeeper: Client environment:java.class.path\u003d/srv/home/rgai0001/hbase-1.2.4/bin/../conf:/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/srv/home/rgai0001/hbase-1.2.4/bin/..:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/activation-1.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/aopalliance-1.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/apacheds-i18n-2.0.0-M15.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/apacheds-kerberos-codec-2.0.0-M15.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/api-asn1-api-1.0.0-M20.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/api-util-1.0.0-M20.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/asm-3.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/avro-1.7.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-beanutils-1.7.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-beanutils-core-1.8.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-cli-1.2.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-codec-1.9.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-collections-3.2.2.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-compress-1.4.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-configuration-1.6.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-daemon-1.0.13.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-digester-1.8.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-el-1.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-httpclient-3.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-io-2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-lang-2.6.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-logging-1.2.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-math-2.2.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-math3-3.1.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/commons-net-3.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/disruptor-3.3.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/findbugs-annotations-1.3.9-1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/guava-12.0.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/guice-3.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/guice-servlet-3.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-annotations-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-auth-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-client-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-common-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-hdfs-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-mapreduce-client-app-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-mapreduce-client-common-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-mapreduce-client-core-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-mapreduce-client-jobclient-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-mapreduce-client-shuffle-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-yarn-api-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-yarn-client-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-yarn-common-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hadoop-yarn-server-common-2.7.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-annotations-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-annotations-1.2.4-tests.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-client-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-common-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-common-1.2.4-tests.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-examples-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-external-blockcache-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-hadoop2-compat-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-hadoop-compat-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-it-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-it-1.2.4-tests.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-prefix-tree-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-procedure-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-protocol-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-resource-bundle-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-rest-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-server-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-server-1.2.4-tests.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-shell-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/hbase-thrift-1.2.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/htrace-core-3.1.0-incubating.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/httpclient-4.2.5.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/httpcore-4.4.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jackson-core-asl-1.9.13.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jackson-jaxrs-1.9.13.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jackson-mapper-asl-1.9.13.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jackson-xc-1.9.13.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jamon-runtime-2.4.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jasper-compiler-5.5.23.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jasper-runtime-5.5.23.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/javax.inject-1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/java-xmlbuilder-0.4.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jaxb-api-2.2.2.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jaxb-impl-2.2.3-1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jcodings-1.0.8.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jersey-client-1.9.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jersey-core-1.9.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jersey-guice-1.9.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jersey-json-1.9.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jersey-server-1.9.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jets3t-0.9.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jettison-1.3.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jetty-6.1.26.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jetty-sslengine-6.1.26.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jetty-util-6.1.26.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/joni-2.1.2.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jruby-complete-1.6.8.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jsch-0.1.42.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jsp-2.1-6.1.14.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/jsp-api-2.1-6.1.14.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/junit-4.12.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/leveldbjni-all-1.8.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/libthrift-0.9.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/log4j-1.2.17.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/metrics-core-2.2.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/netty-all-4.0.23.Final.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/paranamer-2.3.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/protobuf-java-2.5.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/servlet-api-2.5-6.1.14.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/servlet-api-2.5.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/slf4j-api-1.7.7.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/snappy-java-1.0.4.1.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/spymemcached-2.11.6.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/xmlenc-0.52.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/xz-1.0.jar:/srv/home/rgai0001/hbase-1.2.4/bin/../lib/zookeeper-3.4.6.jar:/home/user/hadoop-2.7.3/etc/hadoop:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/user/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/user/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/user/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/user/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/user/hadoop-2.7.3/contrib/capacity-scheduler/*.jar\n2018-05-28 13:21:42,066 INFO  [main] zookeeper.ZooKeeper: Client environment:java.library.path\u003d/home/user/hadoop-2.7.3/lib/native\n2018-05-28 13:21:42,066 INFO  [main] zookeeper.ZooKeeper: Client environment:java.io.tmpdir\u003d/tmp\n2018-05-28 13:21:42,067 INFO  [main] zookeeper.ZooKeeper: Client environment:java.compiler\u003d\u003cNA\u003e\n2018-05-28 13:21:42,067 INFO  [main] zookeeper.ZooKeeper: Client environment:os.name\u003dLinux\n2018-05-28 13:21:42,067 INFO  [main] zookeeper.ZooKeeper: Client environment:os.arch\u003damd64\n2018-05-28 13:21:42,067 INFO  [main] zookeeper.ZooKeeper: Client environment:os.version\u003d4.4.0-121-generic\n2018-05-28 13:21:42,067 INFO  [main] zookeeper.ZooKeeper: Client environment:user.name\u003drgai0001\n2018-05-28 13:21:42,068 INFO  [main] zookeeper.ZooKeeper: Client environment:user.home\u003d/srv/home/rgai0001\n2018-05-28 13:21:42,068 INFO  [main] zookeeper.ZooKeeper: Client environment:user.dir\u003d/home/user\n2018-05-28 13:21:42,069 INFO  [main] zookeeper.ZooKeeper: Initiating client connection, connectString\u003dlocalhost:2181 sessionTimeout\u003d90000 watcher\u003dhconnection-0x1165b380x0, quorum\u003dlocalhost:2181, baseZNode\u003d/hbase\n2018-05-28 13:21:42,086 INFO  [main-SendThread(localhost:2181)] zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n2018-05-28 13:21:42,102 INFO  [main-SendThread(localhost:2181)] zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session\n2018-05-28 13:21:42,112 INFO  [main-SendThread(localhost:2181)] zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid \u003d 0x163a45b10650020, negotiated timeout \u003d 90000\n2018-05-28 13:21:43,081 INFO  [main] Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n2018-05-28 13:21:43,134 INFO  [main] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid\u003d0x163a45b10650020\n2018-05-28 13:21:43,137 INFO  [main] zookeeper.ZooKeeper: Session: 0x163a45b10650020 closed\n2018-05-28 13:21:43,137 INFO  [main-EventThread] zookeeper.ClientCnxn: EventThread shut down\n2018-05-28 13:21:43,162 INFO  [main] Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n2018-05-28 13:21:43,163 INFO  [main] jvm.JvmMetrics: Initializing JVM Metrics with processName\u003dJobTracker, sessionId\u003d\n2018-05-28 13:21:43,195 INFO  [main] Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n2018-05-28 13:21:43,454 INFO  [main] input.FileInputFormat: Total input paths to process : 1\n2018-05-28 13:21:43,503 INFO  [main] mapreduce.JobSubmitter: number of splits:1\n2018-05-28 13:21:43,509 INFO  [main] Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n2018-05-28 13:21:43,601 INFO  [main] mapreduce.JobSubmitter: Submitting tokens for job: job_local1768126181_0001\n2018-05-28 13:21:44,079 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527477703688/hbase-common-1.2.4.jar \u003c- /home/user/hbase-common-1.2.4.jar\n2018-05-28 13:21:44,086 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/hbase-common-1.2.4.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703688/hbase-common-1.2.4.jar\n2018-05-28 13:21:44,086 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527477703689/hbase-prefix-tree-1.2.4.jar \u003c- /home/user/hbase-prefix-tree-1.2.4.jar\n2018-05-28 13:21:44,089 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/hbase-prefix-tree-1.2.4.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703689/hbase-prefix-tree-1.2.4.jar\n2018-05-28 13:21:44,089 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527477703690/hbase-protocol-1.2.4.jar \u003c- /home/user/hbase-protocol-1.2.4.jar\n2018-05-28 13:21:44,092 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/hbase-protocol-1.2.4.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703690/hbase-protocol-1.2.4.jar\n2018-05-28 13:21:44,092 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527477703691/hbase-client-1.2.4.jar \u003c- /home/user/hbase-client-1.2.4.jar\n2018-05-28 13:21:44,094 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/hbase-client-1.2.4.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703691/hbase-client-1.2.4.jar\n2018-05-28 13:21:44,095 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527477703692/zookeeper-3.4.6.jar \u003c- /home/user/zookeeper-3.4.6.jar\n2018-05-28 13:21:44,097 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/zookeeper-3.4.6.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703692/zookeeper-3.4.6.jar\n2018-05-28 13:21:44,097 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527477703693/hbase-hadoop-compat-1.2.4.jar \u003c- /home/user/hbase-hadoop-compat-1.2.4.jar\n2018-05-28 13:21:44,099 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/hbase-hadoop-compat-1.2.4.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703693/hbase-hadoop-compat-1.2.4.jar\n2018-05-28 13:21:44,099 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527477703694/protobuf-java-2.5.0.jar \u003c- /home/user/protobuf-java-2.5.0.jar\n2018-05-28 13:21:44,100 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/protobuf-java-2.5.0.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703694/protobuf-java-2.5.0.jar\n2018-05-28 13:21:44,101 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527477703695/htrace-core-3.1.0-incubating.jar \u003c- /home/user/htrace-core-3.1.0-incubating.jar\n2018-05-28 13:21:44,102 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/htrace-core-3.1.0-incubating.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703695/htrace-core-3.1.0-incubating.jar\n2018-05-28 13:21:44,102 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527477703696/hadoop-mapreduce-client-core-2.7.3.jar \u003c- /home/user/hadoop-mapreduce-client-core-2.7.3.jar\n2018-05-28 13:21:44,103 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/hadoop-mapreduce-client-core-2.7.3.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703696/hadoop-mapreduce-client-core-2.7.3.jar\n2018-05-28 13:21:44,104 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527477703697/guava-12.0.1.jar \u003c- /home/user/guava-12.0.1.jar\n2018-05-28 13:21:44,105 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/guava-12.0.1.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703697/guava-12.0.1.jar\n2018-05-28 13:21:44,105 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527477703698/metrics-core-2.2.0.jar \u003c- /home/user/metrics-core-2.2.0.jar\n2018-05-28 13:21:44,107 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/metrics-core-2.2.0.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703698/metrics-core-2.2.0.jar\n2018-05-28 13:21:44,107 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527477703699/netty-all-4.0.23.Final.jar \u003c- /home/user/netty-all-4.0.23.Final.jar\n2018-05-28 13:21:44,110 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/netty-all-4.0.23.Final.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703699/netty-all-4.0.23.Final.jar\n2018-05-28 13:21:44,110 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527477703700/hbase-server-1.2.4.jar \u003c- /home/user/hbase-server-1.2.4.jar\n2018-05-28 13:21:44,112 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/hbase-server-1.2.4.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703700/hbase-server-1.2.4.jar\n2018-05-28 13:21:44,112 INFO  [main] mapred.LocalDistributedCacheManager: Creating symlink: /home/user/hdfs-working-dirs/tmp/mapred/local/1527477703701/hadoop-common-2.7.3.jar \u003c- /home/user/hadoop-common-2.7.3.jar\n2018-05-28 13:21:44,113 INFO  [main] mapred.LocalDistributedCacheManager: Localized file:/home/user/hbase-1.2.4/lib/hadoop-common-2.7.3.jar as file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703701/hadoop-common-2.7.3.jar\n2018-05-28 13:21:44,161 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703688/hbase-common-1.2.4.jar\n2018-05-28 13:21:44,162 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703689/hbase-prefix-tree-1.2.4.jar\n2018-05-28 13:21:44,162 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703690/hbase-protocol-1.2.4.jar\n2018-05-28 13:21:44,163 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703691/hbase-client-1.2.4.jar\n2018-05-28 13:21:44,163 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703692/zookeeper-3.4.6.jar\n2018-05-28 13:21:44,164 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703693/hbase-hadoop-compat-1.2.4.jar\n2018-05-28 13:21:44,165 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703694/protobuf-java-2.5.0.jar\n2018-05-28 13:21:44,165 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703695/htrace-core-3.1.0-incubating.jar\n2018-05-28 13:21:44,165 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703696/hadoop-mapreduce-client-core-2.7.3.jar\n2018-05-28 13:21:44,165 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703697/guava-12.0.1.jar\n2018-05-28 13:21:44,166 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703698/metrics-core-2.2.0.jar\n2018-05-28 13:21:44,166 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703699/netty-all-4.0.23.Final.jar\n2018-05-28 13:21:44,166 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703700/hbase-server-1.2.4.jar\n2018-05-28 13:21:44,167 INFO  [main] mapred.LocalDistributedCacheManager: file:/home/user/hdfs-working-dirs/tmp/mapred/local/1527477703701/hadoop-common-2.7.3.jar\n2018-05-28 13:21:44,171 INFO  [main] mapreduce.Job: The url to track the job: http://localhost:8080/\n2018-05-28 13:21:44,171 INFO  [main] mapreduce.Job: Running job: job_local1768126181_0001\n2018-05-28 13:21:44,174 INFO  [Thread-26] mapred.LocalJobRunner: OutputCommitter set in config null\n2018-05-28 13:21:44,212 INFO  [Thread-26] Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n2018-05-28 13:21:44,213 INFO  [Thread-26] mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.hbase.mapreduce.TableOutputCommitter\n2018-05-28 13:21:44,258 INFO  [Thread-26] mapred.LocalJobRunner: Waiting for map tasks\n2018-05-28 13:21:44,260 INFO  [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1768126181_0001_m_000000_0\n2018-05-28 13:21:44,313 INFO  [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n2018-05-28 13:21:44,321 INFO  [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: hdfs://localhost:9000/tmp/hbase/salesData.csv:0+130510\n2018-05-28 13:21:44,330 INFO  [LocalJobRunner Map Task Executor #0] zookeeper.RecoverableZooKeeper: Process identifier\u003dhconnection-0x1442b304 connecting to ZooKeeper ensemble\u003dlocalhost:2181\n2018-05-28 13:21:44,330 INFO  [LocalJobRunner Map Task Executor #0] zookeeper.ZooKeeper: Initiating client connection, connectString\u003dlocalhost:2181 sessionTimeout\u003d90000 watcher\u003dhconnection-0x1442b3040x0, quorum\u003dlocalhost:2181, baseZNode\u003d/hbase\n2018-05-28 13:21:44,334 INFO  [LocalJobRunner Map Task Executor #0-SendThread(localhost:2181)] zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n2018-05-28 13:21:44,334 INFO  [LocalJobRunner Map Task Executor #0-SendThread(localhost:2181)] zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session\n2018-05-28 13:21:44,337 INFO  [LocalJobRunner Map Task Executor #0-SendThread(localhost:2181)] zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid \u003d 0x163a45b10650021, negotiated timeout \u003d 90000\n2018-05-28 13:21:44,341 INFO  [LocalJobRunner Map Task Executor #0] mapreduce.TableOutputFormat: Created table instance for sales3\n2018-05-28 13:21:44,372 INFO  [LocalJobRunner Map Task Executor #0] zookeeper.RecoverableZooKeeper: Process identifier\u003dhconnection-0x1168a38 connecting to ZooKeeper ensemble\u003dlocalhost:2181\n2018-05-28 13:21:44,372 INFO  [LocalJobRunner Map Task Executor #0] zookeeper.ZooKeeper: Initiating client connection, connectString\u003dlocalhost:2181 sessionTimeout\u003d90000 watcher\u003dhconnection-0x1168a380x0, quorum\u003dlocalhost:2181, baseZNode\u003d/hbase\n2018-05-28 13:21:44,373 INFO  [LocalJobRunner Map Task Executor #0-SendThread(localhost:2181)] zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n2018-05-28 13:21:44,373 INFO  [LocalJobRunner Map Task Executor #0-SendThread(localhost:2181)] zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session\n2018-05-28 13:21:44,376 INFO  [LocalJobRunner Map Task Executor #0-SendThread(localhost:2181)] zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid \u003d 0x163a45b10650022, negotiated timeout \u003d 90000\n2018-05-28 13:21:44,388 ERROR [LocalJobRunner Map Task Executor #0] mapreduce.DefaultVisibilityExpressionResolver: Error scanning \u0027labels\u0027 table\norg.apache.hadoop.hbase.TableNotFoundException: hbase:labels\n\tat org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1283)\n\tat org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1181)\n\tat org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:305)\n\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:156)\n\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:60)\n\tat org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:210)\n\tat org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:327)\n\tat org.apache.hadoop.hbase.client.ClientScanner.nextScanner(ClientScanner.java:302)\n\tat org.apache.hadoop.hbase.client.ClientScanner.initializeScannerInConstruction(ClientScanner.java:167)\n\tat org.apache.hadoop.hbase.client.ClientScanner.\u003cinit\u003e(ClientScanner.java:162)\n\tat org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:797)\n\tat org.apache.hadoop.hbase.mapreduce.DefaultVisibilityExpressionResolver.init(DefaultVisibilityExpressionResolver.java:91)\n\tat org.apache.hadoop.hbase.mapreduce.CellCreator.\u003cinit\u003e(CellCreator.java:48)\n\tat org.apache.hadoop.hbase.mapreduce.TsvImporterMapper.setup(TsvImporterMapper.java:108)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n2018-05-28 13:21:44,394 INFO  [LocalJobRunner Map Task Executor #0] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid\u003d0x163a45b10650022\n2018-05-28 13:21:44,396 INFO  [LocalJobRunner Map Task Executor #0-EventThread] zookeeper.ClientCnxn: EventThread shut down\n2018-05-28 13:21:44,396 INFO  [LocalJobRunner Map Task Executor #0] zookeeper.ZooKeeper: Session: 0x163a45b10650022 closed\nBad line at offset: 73400:\nExcessive columns\n2018-05-28 13:21:44,535 INFO  [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: \n2018-05-28 13:21:44,758 INFO  [LocalJobRunner Map Task Executor #0] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid\u003d0x163a45b10650021\n2018-05-28 13:21:44,760 INFO  [LocalJobRunner Map Task Executor #0] zookeeper.ZooKeeper: Session: 0x163a45b10650021 closed\n2018-05-28 13:21:44,760 INFO  [LocalJobRunner Map Task Executor #0-EventThread] zookeeper.ClientCnxn: EventThread shut down\n2018-05-28 13:21:44,771 INFO  [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1768126181_0001_m_000000_0 is done. And is in the process of committing\n2018-05-28 13:21:44,777 INFO  [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map\n2018-05-28 13:21:44,777 INFO  [LocalJobRunner Map Task Executor #0] mapred.Task: Task \u0027attempt_local1768126181_0001_m_000000_0\u0027 done.\n2018-05-28 13:21:44,777 INFO  [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1768126181_0001_m_000000_0\n2018-05-28 13:21:44,777 INFO  [Thread-26] mapred.LocalJobRunner: map task executor complete.\n2018-05-28 13:21:45,173 INFO  [main] mapreduce.Job: Job job_local1768126181_0001 running in uber mode : false\n2018-05-28 13:21:45,174 INFO  [main] mapreduce.Job:  map 100% reduce 0%\n2018-05-28 13:21:45,176 INFO  [main] mapreduce.Job: Job job_local1768126181_0001 completed successfully\n2018-05-28 13:21:45,191 INFO  [main] mapreduce.Job: Counters: 21\n\tFile System Counters\n\t\tFILE: Number of bytes read\u003d26187113\n\t\tFILE: Number of bytes written\u003d26757370\n\t\tFILE: Number of read operations\u003d0\n\t\tFILE: Number of large read operations\u003d0\n\t\tFILE: Number of write operations\u003d0\n\t\tHDFS: Number of bytes read\u003d130510\n\t\tHDFS: Number of bytes written\u003d0\n\t\tHDFS: Number of read operations\u003d3\n\t\tHDFS: Number of large read operations\u003d0\n\t\tHDFS: Number of write operations\u003d0\n\tMap-Reduce Framework\n\t\tMap input records\u003d998\n\t\tMap output records\u003d997\n\t\tInput split bytes\u003d110\n\t\tSpilled Records\u003d0\n\t\tFailed Shuffles\u003d0\n\t\tMerged Map outputs\u003d0\n\t\tGC time elapsed (ms)\u003d11\n\t\tTotal committed heap usage (bytes)\u003d62849024\n\tImportTsv\n\t\tBad Lines\u003d1\n\tFile Input Format Counters \n\t\tBytes Read\u003d130510\n\tFile Output Format Counters \n\t\tBytes Written\u003d0\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1527474636140_1026396457",
      "id": "20180528-123036_1720007161",
      "dateCreated": "May 28, 2018 12:30:36 PM",
      "dateStarted": "May 28, 2018 1:21:28 PM",
      "dateFinished": "May 28, 2018 1:21:45 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "#!/bin/bash\n\nhead -n 1 ~/week3/SalesJan2009.csv \u003e ~/week3/header.txt\nawk -F, \u0027{\n    for(i\u003d1; i\u003c\u003dNF; i++)\n      if(i\u003d\u003d1) {printf \"HBASE_ROW_KEY,\"}\n      else if(i\u003d\u003dNF) {printf \"col:%s\", $i}\n      else {printf \"col:%s,\", $i};\n    print NL\n}\u0027 ~/week3/header.txt",
      "user": "anonymous",
      "dateUpdated": "May 27, 2018 9:41:28 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 133.2,
              "optionOpen": false
            }
          }
        },
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "HBASE_ROW_KEY,col:Product,col:Price,col:Payment_Type,col:Name,col:City,col:State,col:Country,col:Account_Created,col:Last_Login,col:Latitude,col:Longitude\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1527416865972_-790506868",
      "id": "20180527-202745_71528779",
      "dateCreated": "May 27, 2018 8:27:45 PM",
      "dateStarted": "May 27, 2018 9:41:28 PM",
      "dateFinished": "May 27, 2018 9:41:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "#!/bin/bash\n\nvar\u003d\"$(ls -l)\"\necho \"$var\"\n",
      "user": "anonymous",
      "dateUpdated": "May 28, 2018 11:56:01 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {
          "val": "",
          "var": "",
          "OUTPUT": ""
        },
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "total 196\n-rwxrwxr-x  1 rgai0001 student   59 Jan 13  2017 124notice.txt\n-rwxr-xr-x  1 rgai0001 student  327 May 27 22:33 22838750_q1.sh\n-rwxr-xr-x  1 rgai0001 student  282 May 27 22:34 22838750_q2.sh\ndrwxrwxr-x  2 rgai0001 student 4096 Jan  3 14:51 AMDAPPSDK-2.9-1\ndrwxr-xr-x  2 rgai0001 student 4096 May 13 21:01 anotherDir\n-rw-r--r--  1 rgai0001 student   97 May 13 21:01 anotherFile.txt\ndrwxrwxr-x  6 rgai0001 student 4096 Feb  2  2017 BAK\ndrwxrwxr-x  2 rgai0001 student 4096 Nov  3  2017 bin\n-rw-r--r--  1 rgai0001 student    0 May 10 12:14 Boolean\ndrwxrwxr-x  2 rgai0001 student 4096 Jan  5 17:00 deprecated-sw\n-rwxrwxr-x  1 rgai0001 student  704 May 21 21:02 derby.log\ndrwxrwxr-x  2 rgai0001 student 4096 Feb  2 10:02 Desktop\ndrwxrwxr-x 35 rgai0001 student 4096 Jan  3 14:53 Documents\ndrwxrwxr-x  2 rgai0001 student 4096 Jan  3 16:14 Downloads\ndrwxrwxr-x  3 rgai0001 student 4096 Sep 26  2017 eclipse\ndrwxrwxr-x  8 rgai0001 student 4096 Feb 23  2017 env2\ndrwxrwxr-x 12 rgai0001 student 4096 May 14 19:20 env3\n-rwxrwxr-x  1 rgai0001 student 8980 Nov 10  2016 examples.desktop\ndrwxrwxr-x 10 rgai0001 student 4096 Jan 13  2017 hadoop-2.7.3\ndrwxrwxr-x  9 rgai0001 student 4096 Jan 13  2017 hbase-1.2.4\ndrwxrwxr-x  3 rgai0001 student 4096 Jan 15  2017 hbase-working-dirs\ndrwxrwxr-x  5 rgai0001 student 4096 Jan  6  2017 hdfs-working-dirs\ndrwxrwxr-x  2 rgai0001 student 4096 Aug 25  2017 include\ndrwxrwxr-x  4 rgai0001 student 4096 Jan 13  2017 install\ndrwxrwxr-x  7 rgai0001 student 4096 Nov 28 12:34 kafka_2.11-1.0.0\ndrwxrwxr-x  5 rgai0001 student 4096 May 21 21:02 metastore_db\ndrwxrwxr-x  2 rgai0001 student 4096 Nov  3  2017 mongo-schema\n-rw-r--r--  1 rgai0001 student    0 May 13 21:01 newFile.txt\ndrwxrwxr-x  2 rgai0001 student 4096 Oct  2  2017 ori\ndrwxrwxr-x  2 rgai0001 student 4096 Nov 10  2016 Pictures\ndrwxrwxr-x  2 rgai0001 student 4096 Nov 10  2016 Public\ndrwxrwxr-x  4 rgai0001 student 4096 Jan 13  2017 sbt\ndrwxrwxr-x  5 rgai0001 student 4096 Aug 26  2017 scala-visualisation\ndrwxrwxr-x 12 rgai0001 student 4096 Nov 25  2017 spark-2.2.1-bin-hadoop2.7\ndrwxrwxr-x  2 rgai0001 student 4096 Jan  5  2017 spark-warehouse\ndrwxrwxr-x  7 rgai0001 student 4096 Sep 21  2017 src\n-rwxr-xr-x  1 rgai0001 student  407 May 18 11:57 start-jupyter.sh\n-rwxr-xr-x  1 rgai0001 student  345 May 20 15:40 start-zeppelin.sh\n-rwxr-xr-x  1 rgai0001 student   37 May 20 15:40 stop-zeppelin.sh\ndrwxrwxr-x  2 rgai0001 student 4096 Jan 20  2017 streamdemo\ndrwxrwxr-x  2 rgai0001 student 4096 Jan 11  2017 target\ndrwxrwxr-x  2 rgai0001 student 4096 Nov 10  2016 Templates\ndrwxrwxr-x  4 rgai0001 student 4096 Jan  2 17:12 tmp\ndrwxrwxr-x  2 rgai0001 student 4096 Nov 10  2016 Videos\ndrwxr-xr-x  3 rgai0001 student 4096 May 14 19:17 week1\ndrwxr-xr-x  6 rgai0001 student 4096 May 23 10:02 week2\ndrwxr-xr-x  2 rgai0001 student 4096 May 28 11:51 week3\n-rw-r--r--  1 rgai0001 student 2426 May 23 10:00 WordCountOldAPI.scala\ndrwxrwxr-x 13 rgai0001 student 4096 May 27 22:48 zeppelin-0.7.3-bin-all\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1527418199596_274726564",
      "id": "20180527-204959_162110473",
      "dateCreated": "May 27, 2018 8:49:59 PM",
      "dateStarted": "May 28, 2018 11:56:01 AM",
      "dateFinished": "May 28, 2018 11:56:01 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "#!/bin/bash\n\n# template for Assignment 1, first file\necho \"Ralph Michael Gailis\"\necho \"22838750\"\necho \"1. Create a folder on the HDFS and name it after your\nstudent number: /tmp/\u003cstudent_number\u003e\"\n\n# write your code here (uncomment the line!). For example:\nhadoop fs -mkdir -p /tmp/22838750\n\n# add comments using echo command:\necho \"An example of irrelevant comment!\"",
      "user": "anonymous",
      "dateUpdated": "May 28, 2018 12:29:22 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1527416923146_1906826464",
      "id": "20180527-202843_2060997321",
      "dateCreated": "May 27, 2018 8:28:43 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "#!/bin/bash\n\n# template for Assignment 1, second file\necho \"Ralph Michael Gailis\"\necho \"22838750\"\n\necho \"1. Create a folder on the HDFS and name it after your student number:\"\n# create a folder on hdfs\nhadoop fs -mkdir -p /tmp/22838750\n\necho \"2. List all the files in that folder.\"\n# listing files in hdfs\nhadoop fs -ls -R\n\necho \"3. Transfer the above file to the HDFS folder that you just created (use absolute paths).\"\n# Transferring a file to hdfs using put\nhadoop fs -put 22838750_q1.sh /tmp/22838750\n\necho \"4. Print 10 lines (with any preferred order) of the the transferred file which is now stored on HDFS.\"\n# Print 10 lines of transferred file\nhadoop fs -cat /tmp/22838750/22838750_q1.sh | head -n 10\n\necho \"\nThe End!\n\"\n",
      "user": "anonymous",
      "dateUpdated": "May 28, 2018 3:01:41 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1527424080615_-2865170",
      "id": "20180527-222800_1261061409",
      "dateCreated": "May 27, 2018 10:28:00 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "hbase-script",
  "id": "2DFJWUGNK",
  "angularObjects": {
    "2CY4SB1H5:shared_process": [],
    "2CXC618EC:shared_process": [],
    "2CYVBGAMQ:shared_process": [],
    "2CY3Y61TM:shared_process": [],
    "2CXRH3T8B:shared_process": [],
    "2CXTQKGUW:shared_process": [],
    "2CXCNM6ZT:shared_process": [],
    "2CXZADFKB:shared_process": [],
    "2CUZKD23K:shared_process": [],
    "2CVM42UGQ:shared_process": [],
    "2CVPRSQB6:shared_process": [],
    "2CYHQDYMU:shared_process": [],
    "2CVV5NFCM:shared_process": [],
    "2CW23H68H:shared_process": [],
    "2CY8N5QUM:shared_process": [],
    "2CVTBZ9KW:shared_process": [],
    "2CWGDUPZW:shared_process": [],
    "2CVKDDERZ:shared_process": [],
    "2CYGMRJ4Q:shared_process": []
  },
  "config": {},
  "info": {}
}